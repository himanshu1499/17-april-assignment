{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a57c02-8222-4c57-8b3f-ea59fe3ec26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197057e-ffd0-4ee8-bc04-5726e84d9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting is a machine learning algorithm that uses an ensemble of decision trees to make predictions. The algorithm works by iteratively adding decision trees to the ensemble, each one correcting the errors of the previous trees. The core concept behind Gradient Boosting is to minimize a loss function by gradient descent. Here are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm:\n",
    "\n",
    "Define a loss function: The first step is to define a loss function that measures how well the model fits the data. In Gradient Boosting, the most commonly used loss function is the mean squared error (MSE).\n",
    "\n",
    "Initialize the model: The second step is to initialize the model with a constant value, such as the mean of the target variable. This constant value represents the average prediction of the model for all the data points.\n",
    "\n",
    "Calculate the residuals: The next step is to calculate the residuals, which are the differences between the predicted values and the actual values of the target variable. These residuals represent the errors of the current model.\n",
    "\n",
    "Train a decision tree on the residuals: The fourth step is to train a decision tree on the residuals. The decision tree is trained to predict the residuals of the current model, rather than the target variable itself.\n",
    "\n",
    "Update the model: The fifth step is to update the model by adding the output of the decision tree to the current predictions. This updated model is the sum of the previous model and the output of the new decision tree.\n",
    "\n",
    "Repeat steps 3 to 5: The sixth step is to repeat steps 3 to 5 until a stopping criterion is met. This stopping criterion can be a maximum number of iterations, a minimum improvement in the loss function, or any other criterion that makes sense for the problem.\n",
    "\n",
    "Make predictions: The final step is to use the trained model to make predictions on new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e055cd7-4bc0-4cf4-8c0c-52fd460759bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879d9285-f953-4af2-b7ab-caea5df589fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.mean = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "        F = np.full(y.shape, self.mean)\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - F\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        F = np.full(X.shape[0], self.mean)\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        return F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae7852-9a3d-4313-9e1f-57858900b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "This implementation includes the following steps:\n",
    "\n",
    "Initialization of the hyperparameters, such as the number of estimators, the maximum depth of the decision trees, and the learning rate.\n",
    "Definition of a fit method that iteratively trains decision trees on the residuals and updates the predictions using the learning rate. The mean of the target variable is used as the initial prediction.\n",
    "Definition of a predict method that uses the trained trees to make predictions on new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7518cc66-f34a-4d1b-b27c-2a0b165e826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 6.10\n",
      "R-squared: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2e117-7a6a-4028-8f39-6d030d3ca817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941b91b7-64b3-4f1c-9617-c96e70c1732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}\n",
      "MSE: 9.40\n",
      "R-squared: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(gb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01007eaa-644b-408b-bdca-3ee1850c3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, we use GridSearchCV from sklearn.model_selection library to search for the best hyperparameters of our gradient boosting model.\n",
    "We define a dictionary of hyperparameters and their values, and GridSearchCV performs an exhaustive search over all possible combinations of hyperparameters.\n",
    "\n",
    "We also specify the number of cross-validation folds to use (cv=5), the scoring metric to use (neg_mean_squared_error), and the model to use (GradientBoostingRegressor). \n",
    "After the grid search is completed, we print out the best hyperparameters found by the algorithm. \n",
    "\n",
    "Finally, we use the best hyperparameters to train the model on the training data, and evaluate its performance on the test data using MSE and R-squared metrics. \n",
    "\n",
    "You can also use RandomizedSearchCV from sklearn.model_selection library to perform a randomized search over a range of hyperparameters instead of an exhaustive grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ca2b9-d44a-4e69-b20c-9bc90df0ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe9de6-5dba-4d20-9d7e-cf23871c515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner is a simple model that performs only slightly better than random guessing on a classification or regression problem.\n",
    "The weak learner can be any algorithm that produces an output based on the input features, such as decision trees or linear models.\n",
    "\n",
    "In the context of Gradient Boosting, weak learners are iteratively combined to form a strong learner that can accurately predict the target variable. \n",
    "At each iteration, the weak learner is trained on the residual errors of the previous model, and its predictions are added to the previous model's predictions to update the overall model.\n",
    "This process continues until the model reaches a predefined stopping criterion or until a maximum number of iterations is reached.\n",
    "\n",
    "The reason for using weak learners in Gradient Boosting is that they allow for a more stable and efficient training process. \n",
    "By using simple models, the algorithm can focus on learning the complex relationships between the input features and the target variable, without overfitting to the training data. \n",
    "The weak learners also prevent the algorithm from getting stuck in local optima, which can occur when using a single complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf912156-2214-49c9-97ed-b0e59744c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefcc9e-30a2-4548-b6da-a692a806963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind Gradient Boosting is to iteratively add weak learners to a model in order to improve its predictions.\n",
    "\n",
    "The algorithm works by starting with an initial prediction for each data point, which can be a simple average of the target variable for regression problems or the most frequent class for classification problems.\n",
    "Then, it trains a weak learner, typically a decision tree with a small number of nodes, to predict the difference between the true target values and the initial predictions. This difference is called the \"residual error\".\n",
    "\n",
    "\n",
    "The weak learner is trained on the residual errors rather than the original target values, which allows it to focus on the patterns in the data that are not captured by the initial predictions.\n",
    "The weak learner's predictions are then added to the initial predictions to create an updated set of predictions.\n",
    "\n",
    "This process is repeated iteratively, with each new weak learner trained on the residual errors of the previous model. \n",
    "The idea is that each new weak learner focuses on the errors that the previous models were unable to capture, leading to a more accurate prediction overall.\n",
    "\n",
    "The term \"gradient\" in Gradient Boosting refers to the use of the gradient of the loss function, such as mean squared error or cross-entropy, to guide the training process. The gradient is used to determine the direction and magnitude of the update to the predictions at each step, which allows the algorithm to converge to the optimal solution in a more efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db2bca-9cbf-4981-ac44-6a6edb2483e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a6867-5f29-4094-9225-d6d3e208e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting builds an ensemble of weak learners by iteratively adding them to the model. Each weak learner is trained on the residual errors of the previous model, and its predictions are added to the previous model's predictions to update the overall model.\n",
    "\n",
    "The process of building the ensemble can be broken down into the following steps:\n",
    "\n",
    "Initialize the model with an initial prediction for each data point. For regression problems, this can be a simple average of the target variable, and for classification problems, this can be the most frequent class.\n",
    "\n",
    "Calculate the residual errors between the initial predictions and the true target values.\n",
    "\n",
    "Train a weak learner, typically a decision tree with a small number of nodes, to predict the residual errors.\n",
    "\n",
    "Add the weak learner's predictions to the previous model's predictions to update the overall model.\n",
    "\n",
    "Repeat steps 2-4 until the model reaches a predefined stopping criterion or a maximum number of iterations is reached.\n",
    "\n",
    "The weak learners in Gradient Boosting are called \"weak\" because they are typically simple models that perform only slightly better than random guessing. However, by combining many weak learners together, Gradient Boosting can create a strong and flexible ensemble that can accurately predict the target variable.\n",
    "\n",
    "The use of an ensemble in Gradient Boosting also helps to reduce overfitting, as the individual weak learners are trained on different subsets of the data and focus on different aspects of the problem. This helps to ensure that the model is able to generalize well to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
